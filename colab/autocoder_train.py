{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"autocoder_train.py","provenance":[{"file_id":"1Z-80RWWcdKK-nYk8P4rYKIJ-7czaJ2hp","timestamp":1634440680315}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPz4Dqr3XpGxFJWEMlRe28R"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bWRSvLuGNEhy","executionInfo":{"status":"ok","timestamp":1637612473384,"user_tz":480,"elapsed":2254,"user":{"displayName":"David Brynjar Franzson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQEgWbmHtGqZoMA8uXFU7zxZSmdJoRh27L6dmO-g=s64","userId":"09928328139330373823"}}},"source":["#############################\n","#          IMPORTS          #\n","#############################\n","import sys\n","import numpy as np\n","import time\n","import math\n","from typing import List, Any\n","import os\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","#from tensorflow.python.keras import backend as K\n","import tensorflow.keras.backend as K\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liaczEyyIuoS","executionInfo":{"status":"ok","timestamp":1637612493519,"user_tz":480,"elapsed":20140,"user":{"displayName":"David Brynjar Franzson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQEgWbmHtGqZoMA8uXFU7zxZSmdJoRh27L6dmO-g=s64","userId":"09928328139330373823"}},"outputId":"9ef2f8bc-cb43-460a-e792-67971f060723"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","driveDir = '/content/gdrive/MyDrive/ai_data/'"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"kgTEfBEHLQBH","executionInfo":{"status":"ok","timestamp":1637612466967,"user_tz":480,"elapsed":5,"user":{"displayName":"David Brynjar Franzson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQEgWbmHtGqZoMA8uXFU7zxZSmdJoRh27L6dmO-g=s64","userId":"09928328139330373823"}}},"source":["#filenames = [\"rvk-snd.wav\", \"rvk-env.wav\", \"uae-env.wav\", \"uae-snd.wav\", \"la-env.wav\", \"la-snd.wav\", \"beg-env.wav\", \"comb-env.wav\"]#,\"beach-boys.wav\", \"Cartography_Master_01__44k_16b.wav\", \"tristan-3-min.wav\"]\n","#filenames = [\"halla-singing.wav\"]#,\"beach-boys.wav\", \"Cartography_Master_01__44k_16b.wav\", \"tristan-3-min.wav\"]\n","#filenames = [\"001-A-.wav\", \"002-A-.wav\", \"002-A.wav\", \"003-A-.wav\", \"003-A.wav\", \"003-D#.wav\", \"004-A-.wav\", \"004-A.wav\", \"004-D#.wav\", \"004-E.wav\", \"005-A.wav\", \"005-D#.wav\", \"005-E.wav\", \"006-D#.wav\", \"006-E.wav\", \"007-D#.wav\", \"008-A-.wav\", \"008-B.wav\", \"008-D#.wav\", \"008-F#.wav\", \"008-G-.wav\", \"009-A-.wav\", \"009-D#.wav\", \"009-F#.wav\", \"009-G-.wav\", \"010-A-.wav\", \"010-F#.wav\", \"010-G-.wav\", \"011-A-.wav\", \"011-F#.wav\", \"100-A.wav\", \"100-E.wav\", \"100-G-.wav\"]\n","filenames = [\"004-E.wav\", \"005-A.wav\", \"005-D#.wav\", \"005-E.wav\", \"006-D#.wav\", \"006-E.wav\", \"007-D#.wav\", \"008-A-.wav\", \"008-B.wav\", \"008-D#.wav\", \"008-F#.wav\", \"008-G-.wav\", \"009-A-.wav\", \"009-D#.wav\", \"009-F#.wav\", \"009-G-.wav\", \"010-A-.wav\", \"010-F#.wav\", \"010-G-.wav\", \"011-A-.wav\", \"011-F#.wav\", \"100-A.wav\", \"100-E.wav\", \"100-G-.wav\"]\n","#filenames = [\"enya-001-000.wav\"]"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKw1B7SBOFV9","executionInfo":{"status":"ok","timestamp":1637612467542,"user_tz":480,"elapsed":579,"user":{"displayName":"David Brynjar Franzson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQEgWbmHtGqZoMA8uXFU7zxZSmdJoRh27L6dmO-g=s64","userId":"09928328139330373823"}}},"source":["def import_training_data(input_file):\n","    print(\"... importing training data ...\")\n","    input = np.nan_to_num(np.load(input_file + \".npy\"))\n","    return(input)\n","\n","def rescale(vector, scale_mult, scale_subtract):\n","    return(np.add(np.multiply(vector[0], scale_mult), scale_subtract))\n","\n","def sampling(args):\n","    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n","    # Arguments\n","        args (tensor): mean and log of variance of Q(z|X)\n","    # Returns\n","        z (tensor): sampled latent vector\n","    \"\"\"\n","    z_mean, z_log_var = args\n","    epsilon = 1e-06\n","    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n"," #############################\n"," #       SHALLOW MODEL       #\n"," #############################\n","\n","def init_autoencoder_shallow(input_dim, intermediate_dim, encoded_dim, learning_rate):\n","\n","    input_shape = (input_dim, )\n","    latent_dim = encoded_dim\n","\n","    print(\"input_shape: \", input_shape)\n","\n","    # VAE model = encoder + decoder\n","    # build encoder model\n","    inputs = tf.keras.Input(shape=input_shape, name='encoder_input')\n","    x = tf.keras.layers.Dense(intermediate_dim, activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-5), kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-5))(inputs)\n","\n","    z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n","    z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n","\n","    # use reparameterization trick to push the sampling out as input\n","    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n","    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n","\n","    # instantiate encoder model\n","    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n","    encoder.summary()\n","\n","    # build decoder model\n","    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n","    x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n","    outputs = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x)\n","\n","    # instantiate decoder model\n","    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')\n","    decoder.summary()\n","\n","    # instantiate VAE model\n","    outputs = decoder(encoder(inputs)[2])\n","    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')\n","\n","    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n","\n","    reconstruction_loss *= input_dim\n","    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n","    kl_loss = K.sum(kl_loss, axis=-1)\n","    kl_loss *= -0.5\n","    vae_loss = K.mean(reconstruction_loss + kl_loss)\n","    vae.add_loss(vae_loss)\n","    opt = tf.keras.optimizers.Adam(lr=learning_rate)\n","\n","    vae.compile(optimizer=opt)\n","    return(vae, encoder, decoder)\n","\n"," #############################\n"," #         DEEP MODEL        #\n"," #   (ARBITRARY STRUCTURE)   #\n"," #############################\n","\n","def init_autoencoder_deep(input_dim, intermediate_dim, encoded_dim, learning_rate):\n","\n","    # network parameters\n","    input_shape = (input_dim, )\n","    latent_dim = encoded_dim\n","\n","    # VAE model = encoder + decoder\n","    # build encoder model\n","    inputs = tf.keras.Input(shape=input_shape, name='encoder_input')\n","    x1 = tf.keras.layers.Dense(intermediate_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                        bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                        activity_regularizer=tf.keras.regularizers.l2(1e-5))(inputs)\n","    x2 = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x1)\n","    x3 = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x2)\n","    x4 = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x3)\n","    x5 = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x4)\n","    x6 = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x5)\n","    x7 = tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n","                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),\n","                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x6)\n","    z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x7)\n","    z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x7)\n","\n","    # use reparameterization trick to push the sampling out as input\n","    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n","    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n","\n","    # instantiate encoder model\n","    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n","    encoder.summary()\n","\n","    # build decoder model\n","    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n","    x1 = tf.keras.layers.Dense(16, activation='relu')(latent_inputs)\n","    x2 = tf.keras.layers.Dense(32, activation='relu')(x1)\n","    x3 = tf.keras.layers.Dense(64, activation='relu')(x2)\n","    x4 = tf.keras.layers.Dense(128, activation='relu')(x3)\n","    x5 = tf.keras.layers.Dense(256, activation='relu')(x4)\n","    x6 = tf.keras.layers.Dense(512, activation='relu')(x5)\n","    x7 = tf.keras.layers.Dense(intermediate_dim, activation='relu')(x6)\n","    outputs = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x7)\n","\n","    # instantiate decoder model\n","    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')\n","    decoder.summary()\n","\n","    # instantiate VAE model\n","    outputs = decoder(encoder(inputs)[2])\n","    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')\n","\n","    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n","    reconstruction_loss *= input_dim\n","    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n","    kl_loss = K.sum(kl_loss, axis=-1)\n","    kl_loss *= -0.5\n","    vae_loss = K.mean(reconstruction_loss + kl_loss)\n","    vae.add_loss(vae_loss)\n","    opt = tf.keras.optimizers.Adam(lr=learning_rate)\n","    vae.compile(optimizer=opt)\n","    return(vae, encoder, decoder)\n","\n"," #############################\n"," #       TRAINING LOOP       #\n"," #############################\n","\n","def train(filename, vae, encoder, decoder, input, min_delta, regression_patience = 1, batch_size = 4096, deep = 0, filename_out=\"blah.wav\"):\n","\n","    tf.executing_eagerly()\n","    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=min_delta, patience = regression_patience)\n","    history = vae.fit(input,\n","            batch_size = batch_size,\n","            epochs=50000, verbose = 1, callbacks=[es])\n","\n","    vae.save_weights(filename_out + \".h5\")\n","    scale_mult, scale_subtract = get_minmax(encoder, input)\n","    converter_enc = tf.lite.TFLiteConverter.from_keras_model(encoder)\n","    converter_dec = tf.lite.TFLiteConverter.from_keras_model(decoder)\n","    tflite_model_enc = converter_enc.convert()\n","    tflite_model_dec = converter_dec.convert()\n","\n","    # Save the models\n","    with open(filename_out + '.enc', 'wb') as f:\n","      print(\"   ... writing \", filename_out + '.enc', \"...\")\n","      f.write(tflite_model_enc)\n","    with open(filename_out + '.dec', 'wb') as f:\n","      print(\"   ... writing \", filename_out + '.dec', \"...\")\n","      f.write(tflite_model_dec)\n","\n","    return(vae, encoder, decoder, scale_mult, scale_subtract)\n","\n","\n","\n","#############################\n","#     SCALING FUNCTIONS     #\n","#############################\n","\n","def get_aminmax(X):\n","    min_ = np.amin(X)\n","    max_ = np.amax(X)\n","    return(min_, max_)\n","\n","def scale_array_by_amax(X):\n","    return((X - np.amin(X)) / (np.amax(X) - np.amin(X)))\n","\n","def inverse_rescale(data, scale_mult, scale_subtract):\n","    return(np.divide(np.subtract(data, scale_subtract), scale_mult))\n","\n","###############################\n","# MODEL VARIABLE INPUT/OUTPUT #\n","###############################\n","\n","def get_minmax(encoder, input):\n","    z_encoded = encoder.predict(input)\n","    z_encoded = np.asarray(z_encoded[0], dtype = np.float32)\n","\n","    min = z_encoded.min(axis = 0)\n","    max = z_encoded.max(axis = 0)\n","    scale_mult = np.subtract(max, min)\n","    scale_subtract = min\n","    return(scale_mult, scale_subtract)\n","\n","def write_minmax(filename, minin, maxin):\n","#    output_ = np.zeros([1, 2])\n","#    output_[0][0] = minin\n","#    output_[0][1] = maxin\n","#    np.savetxt(filename + \".minmax\", output_, delimiter = \", \")#, fmt=\"%1.6f\")\n","    np.savetxt(filename + \".minmax\", np.asarray([[minin, maxin]]), delimiter = \", \")#, fmt=\"%1.6f\")\n","\n","def read_minmax(filename):\n","    input_ = np.loadtxt(filename + \".minmax\", delimiter = \", \")\n","    return(float(input_[0]), float(input_[1]))\n","\n","def write_mm(filename, minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim, deep):\n","    output_ = np.zeros([3, encoded_dim])\n","    output_[0] = scale_mult\n","    output_[1] = scale_subtract\n","    output_[2][0] = minin\n","    output_[2][1] = maxin\n","    output_[2][2] = input_dim\n","    output_[2][3] = intermediate_dim\n","    output_[2][4] = encoded_dim\n","    output_[2][5] = deep\n","    np.savetxt(filename + \".mm\", output_, delimiter = \", \", fmt=\"%1.6f\")\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1IAikenpN-MBlk_s0yagGbMjVDzHBTLB0"},"id":"jeV2yo3gKpac","executionInfo":{"status":"ok","timestamp":1637622462261,"user_tz":480,"elapsed":4388245,"user":{"displayName":"David Brynjar Franzson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQEgWbmHtGqZoMA8uXFU7zxZSmdJoRh27L6dmO-g=s64","userId":"09928328139330373823"}},"outputId":"d181ff99-0555-46a8-a13f-3ac75e8cff1f"},"source":["\n","#############################\n","#    EDITABLE PARAMETERS    #\n","#############################\n","at = time.time()\n","windowskip = 1024    # TRAINING, NOT GENERATOR, GENERATOR IS HARD-CODED TO 4096\n","learning_rate = .00001 #.00001\n","batch_size = 256 #4096    ### SET TO 4 FOR M1, 32 FOR x86 AND 4096 FOR JETSON OR GPU\n","min_delta = .00001 #.00001\n","regression_patience = 100\n","input_dim = 512\n","intermediate_dim = 1000\n","encoded_dim = 8\n","\n","np.set_printoptions(threshold=sys.maxsize)\n","\n","deep = 0\n","\n","print(\"\")\n","print(\"------------------------------\")\n","print(\"|         TRAINING           |\")\n","print(\"------------------------------\")\n","print(\"\")\n","\n","print(tf.__version__)\n","\n","for i in range(0, len(filenames)):  \n","  filename = filenames[i]\n","  filename_ = driveDir + filename\n","\n","  filename_out = filename_ \n","  #  import_training_data(filename_)\n","  #  read_minmax(filename_)\n","  #  train_lite_old(filename_)\n","  #  write_mm(filename_)str\n","\n","  #  # list all data in history\n","  #  print(history.history.keys())\n","  #  # summarize history for loss\n","\n","  input_data = import_training_data(filename_)\n","  minin, maxin = read_minmax(filename_)\n","\n","  if(deep == 0):\n","      vae, encoder, decoder = init_autoencoder_shallow(input_dim, intermediate_dim, encoded_dim, learning_rate)\n","  else:\n","      vae, encoder, decoder = init_autoencoder_deep(input_dim, intermediate_dim, encoded_dim, learning_rate)\n","\n","  vae, encoder, decoder, scale_mult, scale_subtract = train(filename_, vae, encoder, decoder, input_data, min_delta, regression_patience, batch_size, deep, filename_out)\n","\n","  write_mm(filename_out, minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim, deep)\n","\n","print(time.time()-at)\n"," # plt.plot(history.history['loss'])\n"," # plt.title('model loss')\n"," # plt.ylabel('loss')\n"," # plt.xlabel('epoch')\n"," # plt.legend(['train', 'test'], loc='upper left')\n"," # plt.show()\n"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}