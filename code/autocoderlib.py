# THE ANALYZIS FOR SINGLE FRAME MULTI FILE SHOULD BE DONE WITHOUT WINDOWING


#############################
#          IMPORTS          #
#############################
import sys
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
import scipy.io.wavfile
import scipy
from scipy.signal import hann
import math
try:
    #import librosa
    import librosa_load as librosa
except ImportError as e:
    pass
from python_speech_features.base import get_filterbanks
from typing import List, Any
import platform
import os


class color:
   PURPLE = '\033[1;35;48m'
   CYAN = '\033[1;36;48m'
   BOLD = '\033[1;37;48m'
   BLUE = '\033[1;34;48m'
   GREEN = '\033[1;32;48m'
   YELLOW = '\033[1;33;48m'
   RED = '\033[1;31;48m'
   BLACK = '\033[1;30;48m'
   UNDERLINE = '\033[4;37;48m'
   END = '\033[1;37;0m'

np.set_printoptions(threshold=sys.maxsize)

if(len(sys.argv) > 1):
    if(sys.argv[1] == '-api'):
        print()
        print("       import autocoderlib as ac")
        print()
        print("                           ... TFLITE MODEL ...")
        print("       minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim, deep = "+color.CYAN+"ac.read_mm"+color.END+"("+color.GREEN+"input_file.wav"+color.END+")")
        print("       encoder, input_details, output_details = "+color.CYAN+"ac.load_lite"+color.END+"("+color.GREEN+"input_file.wav"+color.END+", \"encoder\")")
        print("       decoder, input_details, output_details = "+color.CYAN+"ac.load_lite"+color.END+"("+color.GREEN+"input_file.wav"+color.END+", \"decoder\")")
        print()
        print("                            ... h5 MODEL ...")
        print("       vae, encoder, decoder = "+color.CYAN+"ac.init_autoencoder_shallow"+color.END+"(input_dim, intermediate_dim, encoded_dim, learning_rate)")
        print("       vae, encoder, decoder = "+color.CYAN+"ac.init_autoencoder_deep"+color.END+"(input_dim, intermediate_dim, encoded_dim, learning_rate)")
        print("       vae, encoder, decoder = "+color.CYAN+"ac.load"+color.END+"("+color.GREEN+"input_file.wav"+color.END+", vae, encoder, decoder)")
        print()
        print("                         ... ENCODE / DECODE ...")
        print("       decoded = "+color.CYAN+"ac.decode"+color.END+"(decoder, deep, scale_mult, scale_subtract, in_vect)")
        print("       encoded = "+color.CYAN+"ac.encode"+color.END+"(encoder, deep, scale_mult, scale_subtract, in_vect)")
        print()
        print("                             ... ANALYSIS ...")
        print("       mel_filter, mel_inversion_filter, window = "+color.CYAN+"ac.initialize"+color.END+"(fftsize, input_dim)")
        print("       data = "+color.CYAN+"ac.readwave"+color.END+"("+color.GREEN+"input_file.wav"+color.END+")")
        print("       minin, maxin = "+color.CYAN+"ac.analyze_data"+color.END+"(data, "+color.GREEN+"input_file.wav"+color.END+", fftsize, windowskip, melsize, window, mel_filter)")
        print()
        print("                             ... TRAINING ...")
        print("       data = "+color.CYAN+"ac.import_training_data"+color.END+"("+color.GREEN+"input_file.wav"+color.END+")")
        print("       see "+color.CYAN+"ac.init_autoencoder_shallow"+color.END+"/"+color.CYAN+"ac.init_autoencoder_deep"+color.END)
        print("       vae, encoder, decoder, scale_mult, scale_subtract = "+color.CYAN+"ac.train"+color.END+"("+color.GREEN+"input_file.wav"+color.END+", vae, encoder, decoder, data, min_delta, regression_patience, get_batch_size(), deep)")
        print("       "+color.CYAN+"ac.write_mm"+color.END+"("+color.GREEN+"input_file"+color.END+", minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim)")
        print()
        print("       For simplicity, input_file.wav is treated as an anchor to load")
        print("       various other files generated by the other functions.")
        print()
        exit()

    if(sys.argv[1] == '-version' or sys.argv[1] == '-v'):
        print()
        print("2021.11.10")
        print()
        exit()

def get_batch_size():

    psystem = platform.system()
    pmachine = platform.machine()
    pprocessor = platform.processor()

    if((psystem=='Darwin') & (pmachine == 'arm64')):
        print("... setting batch size to 4 for m1 mac ...")
        batch_size = 4096

    elif((psystem=='Linux') & (pmachine == 'aarch64')):
        print("... setting batch size to 4096 for nvidia jetson ...")
        batch_size = 4096

    else:
        print("... setting batch size to 32 for a generic system ...")
        batch_size = 8192

    return(batch_size)

#############################
#          MEL CODE         #
#############################

def create_mel_filter(fft_size, n_freq_components = 64, start_freq = 300, end_freq = 8000, samplerate = 44100):
    filterbanks = get_filterbanks(nfilt=n_freq_components,
                                           nfft=fft_size, samplerate=samplerate,
                                           lowfreq=start_freq, highfreq=end_freq)
    mel_inversion_filter = np.ascontiguousarray((filterbanks.T[0:(int(fft_size/2))]).T)
    mel_filter = np.ascontiguousarray(np.divide(mel_inversion_filter.T, mel_inversion_filter.sum(axis=1)))

    return mel_filter, mel_inversion_filter

def spectrogram_to_mel(spectrogram, filter):
    mel_spec = np.transpose(filter).dot(np.transpose(spectrogram))
    return mel_spec

def mel_to_spectrogram(mel_spec, filter):
    mel_spec = np.add(mel_spec, 2.)
    mel_spec = mel_spec.dot(filter)
    mel_spec = np.subtract(np.divide(mel_spec, 2.), 1.)
    return mel_spec

#############################
#       ANALYSIS CODE       #
#############################

def convertToBin(data):
    return(np.sqrt(np.add(np.multiply(data.real, data.real), np.multiply(data.imag, data.imag))))

def convertToPhase(data):
    return(np.angle(data))

def readwave(filename):
    wavefile = librosa.load(filename, sr = 44100, mono = True)
    print("  ####################################")
    print("  #   number of samples:  ", len(wavefile[0]))
    print("  ####################################")
    print("")
    return(wavefile[0])

def initialize(size, melsize):
    window = np.zeros((1, size))
    window = np.ascontiguousarray(window)
    window[0,] = hann(size)
    mel_filter, mel_inversion_filter = create_mel_filter(size, melsize, 0, 22050, 44100)
    np.nan_to_num(mel_filter, False, nan = 0.0)
    np.nan_to_num(mel_inversion_filter, False, nan = 0.0)
    return(mel_filter, mel_inversion_filter, window)

def analyze(data, window, mel_filter):
    data = np.multiply(data, window)
    fftdata = scipy.fft.rfft(data)
    ampslize = convertToBin(fftdata)
    ampslize = np.ascontiguousarray(ampslize)
    #phase = np.angle(fftdata)
    melslize = spectrogram_to_mel(ampslize[0,0:int(data.shape[1]/2)], mel_filter)
    return(melslize, ampslize[0,0:int(data.shape[1]/2)])

def analyze_with_phase(data, window, mel_filter):
    data = np.multiply(data, window)
    fftdata = scipy.fft.rfft(data)
    ampslize = convertToBin(fftdata)
    ampslize = np.ascontiguousarray(ampslize)
    phase = np.angle(fftdata)
    melslize = spectrogram_to_mel(ampslize[0,0:int(data.shape[1]/2)], mel_filter)
    return(melslize, phase[0])

def analyze_normalized(data, window, mel_filter):
    data = np.multiply(data, window)
    fftdata = scipy.fft.rfft(data)
    ampslize = convertToBin(fftdata)
    ampslize = np.ascontiguousarray(ampslize)
    norm_factor = np.amax(ampslize)
    if(norm_factor != 0):
        ampslize = np.divide(ampslize, norm_factor)
    phase = np.angle(fftdata)
    melslize = spectrogram_to_mel(ampslize[0,0:int(data.shape[1]/2)], mel_filter)
    return(melslize, phase, norm_factor)

def analyze_data(data, filename, fftsize, windowskip, melsize, window, mel_filter):

    n_slizes = round(len(data)/windowskip)
    output = np.zeros((int((n_slizes - 16))+1, melsize))
    output = np.ascontiguousarray(output)
    fft_output = np.zeros((int((n_slizes - 16))+1, int(fftsize / 2)))
    fft_output = np.ascontiguousarray(fft_output)

    in_slize = np.zeros((1, fftsize))
    in_slize = np.ascontiguousarray(in_slize)
    for i in range(0, (n_slizes - 16)):
        in_slize[0] = data[i * windowskip:((i*windowskip) + fftsize)]
        output[i,:], fft_output[i,:] = analyze(in_slize, window, mel_filter)

    output = np.nan_to_num(output, 0.)
    minin, maxin = get_aminmax(output)
    output = scale_array_by_amax(output)

    np.save(filename + ".npy", output)
    #np.save(filename + ".fft.npy", fft_output)
    return(minin, maxin)

def analyze_data_normalized(data, filename, fftsize, windowskip, melsize, window, mel_filter):

    n_slizes = round(len(data)/windowskip)
    output = np.zeros((int((n_slizes - 16))+1, melsize))
    output = np.ascontiguousarray(output)

    in_slize = np.zeros((1, fftsize))
    in_slize = np.ascontiguousarray(in_slize)
    for i in range(0, (n_slizes - 16)):
        in_slize[0] = data[i * windowskip:((i*windowskip) + fftsize)]
        output[i,:], _, _ = analyze_normalized(in_slize, window, mel_filter)

    output = np.nan_to_num(output, 0.)
    minin, maxin = get_aminmax(output)
    output = scale_array_by_amax(output)

    np.save(filename + ".npy", output)
    return(minin, maxin)



#############################
#       TRAINING CODE       #
#  DERIVED FROM TENSORFLOW  #
#  AND KERAS DOCUMENTATION  #
#############################

#     https://blog.keras.io/building-autoencoders-in-keras.html     #

def import_training_data(input_file):
    print("... importing training data ...")
    input = np.nan_to_num(np.load(input_file + ".npy"))
    return(input)

def rescale(vector, scale_mult, scale_subtract):
    return(np.add(np.multiply(vector[0], scale_mult), scale_subtract))

def sampling(args):
    """Reparameterization trick by sampling from an isotropic unit Gaussian.
    # Arguments
        args (tensor): mean and log of variance of Q(z|X)
    # Returns
        z (tensor): sampled latent vector
    """
    z_mean, z_log_var = args
    epsilon = 1e-06
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

 #############################
 #       SHALLOW MODEL       #
 #############################

def init_autoencoder_shallow(input_dim, intermediate_dim, encoded_dim, learning_rate):

    input_shape = (input_dim, )
    latent_dim = encoded_dim

    print("input_shape: ", input_shape)

    # VAE model = encoder + decoder
    # build encoder model
    inputs = tf.keras.Input(shape=input_shape, name='encoder_input')
    x = tf.keras.layers.Dense(intermediate_dim, activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-5), kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-5))(inputs)

    z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)
    z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)

    # use reparameterization trick to push the sampling out as input
    # note that "output_shape" isn't necessary with the TensorFlow backend
    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

    # instantiate encoder model
    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')
    encoder.summary()

    # build decoder model
    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')
    x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
    outputs = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x)

    # instantiate decoder model
    _,mel_inversion_filter = create_mel_filter(8192, input_dim, 0, 22050, 44100)
    mel = K.expand_dims(tf.constant(mel_inversion_filter), 0)
    transformed_outputs = tf.keras.layers.Dot(axes=(1,1)) ([outputs, mel])
    decoder = tf.keras.Model(latent_inputs, transformed_outputs, name='decoder')
    training_decoder = tf.keras.Model(latent_inputs, outputs, name='training_decoder')
    decoder.summary()
    training_decoder.summary()
    # instantiate VAE model
    training_outputs = training_decoder(encoder(inputs)[2])
    outputs = decoder(encoder(inputs)[2])


    vae = tf.keras.Model(inputs, [training_outputs,outputs], name='vae_mlp')

    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, training_outputs)

    reconstruction_loss *= input_dim
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5
    vae_loss = K.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    opt = tf.keras.optimizers.Adam(lr=learning_rate)

    vae.compile(optimizer=opt)
    return(vae, encoder, decoder, training_decoder)


 #############################
 #         DEEP MODEL        #
 #   (ARBITRARY STRUCTURE)   #
 #############################

def init_autoencoder_deep(input_dim, intermediate_dim, encoded_dim, learning_rate):

    # network parameters
    input_shape = (input_dim, )
    latent_dim = encoded_dim

    # VAE model = encoder + decoder
    # build encoder model
    inputs = tf.keras.Input(shape=input_shape, name='encoder_input')
    x1 = tf.keras.layers.Dense(intermediate_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                        bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                        activity_regularizer=tf.keras.regularizers.l2(1e-5))(inputs)
    x2 = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x1)
    x3 = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x2)
    x4 = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x3)
    x5 = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x4)
    x6 = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x5)
    x7 = tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
                                                    bias_regularizer=tf.keras.regularizers.l2(1e-4),
                                                    activity_regularizer=tf.keras.regularizers.l2(1e-5))(x6)
    z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x7)
    z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x7)

    # use reparameterization trick to push the sampling out as input
    # note that "output_shape" isn't necessary with the TensorFlow backend
    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

    # instantiate encoder model
    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')
    encoder.summary()

    # build decoder model
    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')
    x1 = tf.keras.layers.Dense(16, activation='relu')(latent_inputs)
    x2 = tf.keras.layers.Dense(32, activation='relu')(x1)
    x3 = tf.keras.layers.Dense(64, activation='relu')(x2)
    x4 = tf.keras.layers.Dense(128, activation='relu')(x3)
    x5 = tf.keras.layers.Dense(256, activation='relu')(x4)
    x6 = tf.keras.layers.Dense(512, activation='relu')(x5)
    x7 = tf.keras.layers.Dense(intermediate_dim, activation='relu')(x6)
    outputs = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x7)

    # instantiate decoder model
    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')
    decoder.summary()

    # instantiate VAE model
    outputs = decoder(encoder(inputs)[2])
    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')

    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)
    reconstruction_loss *= input_dim
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5
    vae_loss = K.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    opt = tf.keras.optimizers.Adam(lr=learning_rate)
    vae.compile(optimizer=opt)
    return(vae, encoder, decoder)

 #############################
 #       TRAINING LOOP       #
 #############################

def train(filename, vae, encoder, decoder, training_decoder, input, min_delta, regression_patience = 1, batch_size = 4096, deep = 0):

    tf.executing_eagerly()
    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=min_delta, patience = regression_patience)
    history = vae.fit(input,
            batch_size = batch_size,
            epochs=50000, verbose = 1, callbacks=[es])

    vae.save_weights(filename + ".h5")
    scale_mult, scale_subtract = get_minmax(encoder, input)
    #fft_mult, fft_subtract = get_fft_minmax(vae, input)

    converter_enc = tf.lite.TFLiteConverter.from_keras_model(encoder)
    converter_dec = tf.lite.TFLiteConverter.from_keras_model(decoder)
    converter_training_dec = tf.lite.TFLiteConverter.from_keras_model(training_decoder)
    tflite_model_enc = converter_enc.convert()
    tflite_model_dec = converter_dec.convert()
    tflite_model_training_dec = converter_training_dec.convert()

    # Save the models
    with open(filename + '.enc', 'wb') as f:
      f.write(tflite_model_enc)
    with open(filename + '.fft.dec', 'wb') as f:
      f.write(tflite_model_dec)
    with open(filename + '.dec', 'wb') as f:
      f.write(tflite_model_training_dec)

    return(vae, encoder, decoder, training_decoder, scale_mult, scale_subtract)


def train_converter(filename, ae, input, output, min_delta, regression_patience = 1, batch_size = 4096, deep = 0):

    print("regression patience: ", regression_patience)

    tf.executing_eagerly()
    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=min_delta, patience = regression_patience)
    history = ae.fit(x=input, y=output,
            batch_size = batch_size,
            epochs=50000, verbose = 1, callbacks=[es])

    ae.save_weights(filename + ".fft.h5")
    #scale_mult, scale_subtract = get_minmax(encoder, input)
    #converter_enc = tf.lite.TFLiteConverter.from_keras_model(encoder)
    #converter_dec = tf.lite.TFLiteConverter.from_keras_model(decoder)
    #tflite_model_enc = converter_enc.convert()
    #tflite_model_dec = converter_dec.convert()

    # Save the models
    #with open(filename + '.fft.enc', 'wb') as f:
    #  f.write(tflite_model_enc)
    #with open(filename + '.fft.dec', 'wb') as f:
    #  f.write(tflite_model_dec)

    return(ae)

#############################
#     SCALING FUNCTIONS     #
#############################

def get_aminmax(X):
    return(np.amin(X), np.amax(X))

def scale_array_by_amax(X):
    return((X - np.amin(X)) / (np.amax(X) - np.amin(X)))

def inverse_rescale(data, scale_mult, scale_subtract):
    return(np.divide(np.subtract(data, scale_subtract), scale_mult))

###############################
# MODEL VARIABLE INPUT/OUTPUT #
###############################

def get_minmax(encoder, input):
    z_encoded = encoder.predict(input)
    z_encoded = np.asarray(z_encoded[0], dtype = np.float32)

    min = z_encoded.min(axis = 0)
    max = z_encoded.max(axis = 0)
    scale_mult = np.subtract(max, min)
    scale_subtract = min
    return(scale_mult, scale_subtract)
'''
def get_fft_minmax(vae, input):
    parsed = vae.predict(input)
    parsed = np.asarray(parsed[1], dtype = np.float32)
    print(parsed.shape)

    min = np.amin(parsed)
    max = np.amax(parsed)
    fft_mult = np.subtract(max, min)
    fft_subtract = min
    return(fft_mult, fft_subtract)
'''

def write_minmax(filename, minin, maxin):
#    output_ = np.zeros([1, 2])
#    output_[0][0] = minin
#    output_[0][1] = maxin
#    np.savetxt(filename + ".minmax", output_, delimiter = ", ")#, fmt="%1.6f")
    np.savetxt(filename + ".minmax", np.asarray([[minin, maxin]]), delimiter = ", ")#, fmt="%1.6f")

def read_minmax(filename):
    input_ = np.loadtxt(filename + ".minmax", delimiter = ", ")
    return(float(input_[0]), float(input_[1]))

def write_mm(filename, minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim, deep):
    output_ = np.zeros([3, 8])
    output_[0] = scale_mult
    output_[1] = scale_subtract
    output_[2][0] = minin
    output_[2][1] = maxin
    output_[2][2] = input_dim
    output_[2][3] = intermediate_dim
    output_[2][4] = encoded_dim
    output_[2][5] = deep
    np.savetxt(filename + ".mm", output_, delimiter = ", ", fmt="%1.6f")

###################################
#      LOAD, ENCODE, DECODE       #
###################################

def read_mm(filename):
    input_ = np.loadtxt(filename + ".mm", delimiter = ", ")
    minin = float(input_[2][0])
    maxin = float(input_[2][1])
    scale_mult = input_[0]
    scale_subtract = input_[1]
    input_dim = int(input_[2][2])
    intermediate_dim = int(input_[2][3])
    encoded_dim = int(input_[2][4])
    deep = int(input_[2][5])
    return(minin, maxin, scale_mult, scale_subtract, input_dim, intermediate_dim, encoded_dim, deep)

def load(filename_, vae, encoder, decoder):
    vae.load_weights(filename_ + ".h5")
    opt = tf.keras.optimizers.Adam(lr=.0001)
    vae.compile(optimizer=opt)
    return(vae, encoder, decoder)

def load_lite(filename_, type):    # SIMPLY CALL IT WITH EITHER THE ENCODER OR DEODER FILENAME
    if(type == "encoder"):
        ext = ".enc"
    else:
        ext = ".dec"
    interpreter = tf.lite.Interpreter(model_path=filename_ + ext)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    return(interpreter, input_details, output_details)

def decode(interpreter, deep, scale_mult, scale_subtract, *args: List[Any]):   # decoder, encoded_dim, scale_mult, scale_subtract, data
    layer_index = 7
    if(deep == 1):
        layer_index = 25
    elif(deep == 2):
        layer_index = 13
    return(code(interpreter, deep, layer_index, rescale(args[0], scale_mult, scale_subtract).astype(np.float32)))

def encode(interpreter, deep, scale_mult, scale_subtract, *args:List[Any]):
    return(inverse_rescale(code(interpreter, deep, 15 if (deep == 0) else 29, args[0]), scale_mult, scale_subtract))

def code(interpreter, deep, last_layer, *args: List[Any]):
    # LAST_LAYER MAY NEED TO BE ADJUSTED FOR DEEP
    if(type(interpreter) == tf.lite.Interpreter):
        interpreter.set_tensor(0, [args[0].astype(np.float32)])   # 0: FIRST LAYER
        interpreter.invoke()
        output_data = interpreter.get_tensor(last_layer)[0] # 7: LAST LAYER, NEEDS TO BE UPDATED FOR DEEP OR OTHER SHAPES
    else:
        output_data = interpreter.predict([args[0].astype(np.float32)])

    return(output_data)
